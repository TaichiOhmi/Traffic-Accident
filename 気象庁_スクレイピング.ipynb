{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "44bab6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"京都.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=61&block_no=47759&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "da2b69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"滋賀.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=60&block_no=47761&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "770ed542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"三重.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=53&block_no=47651&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "616e51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"愛知.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=51&block_no=47636&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3bfa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"静岡.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=50&block_no=47656&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5b938dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"岐阜.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=52&block_no=47632&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "66032bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"長野.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=48&block_no=47610&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "75bd6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"山梨.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=49&block_no=47638&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c30f9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"福井.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=57&block_no=47616&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a7246775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"石川.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=56&block_no=47605&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2fdecc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"新潟.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=54&block_no=47604&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bb74e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"北海道_札幌.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=14&block_no=47412&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a92bf1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"北海道_函館.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=23&block_no=47430&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "014510fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"北海道_旭川.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=12&block_no=47407&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b0f73d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"北海道_釧路.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=19&block_no=47418&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ad57fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"北海道_北見.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=17&block_no=47409&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e69dab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"山口.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=81&block_no=47784&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b72ef01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"広島.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=67&block_no=47765&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bd55beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"岡山.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=66&block_no=47768&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "33097e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"徳島.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=71&block_no=47895&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3d504f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"愛媛.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=73&block_no=47887&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a69c3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"宮崎.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=87&block_no=47830&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a6c84fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"山形.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=35&block_no=47588&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "06e83948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"沖縄.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=91&block_no=47936&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c25403d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"栃木.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=41&block_no=47615&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b34737d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"静岡.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=50&block_no=47656&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c90c5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"鳥取.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=69&block_no=47746&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4c55f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def str2float(weather_data):\n",
    "    try:\n",
    "        return float(weather_data)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def scraping(url, date):\n",
    "\n",
    "    # 気象データのページを取得\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    trs = soup.find(\"table\", { \"class\" : \"data2_s\" })\n",
    "\n",
    "    data_list = []\n",
    "    data_list_per_hour = []\n",
    "\n",
    "    # table の中身を取得\n",
    "    for tr in trs.findAll('tr')[2:]:\n",
    "        tds = tr.findAll('td')\n",
    "\n",
    "        if tds[1].string == None:\n",
    "            break;\n",
    "\n",
    "        data_list.append(date)\n",
    "        data_list.append(tds[0].string)\n",
    "        data_list.append(str2float(tds[1].string))\n",
    "        data_list.append(str2float(tds[2].string))\n",
    "        data_list.append(str2float(tds[3].string))\n",
    "        data_list.append(str2float(tds[4].string))\n",
    "        data_list.append(str2float(tds[5].string))\n",
    "        data_list.append(str2float(tds[6].string))\n",
    "        data_list.append(str2float(tds[7].string))\n",
    "        data_list.append(str2float(tds[8].string))\n",
    "        data_list.append(str2float(tds[9].string))\n",
    "        data_list.append(str2float(tds[10].string))\n",
    "        data_list.append(str2float(tds[11].string))\n",
    "        data_list.append(str2float(tds[12].string))\n",
    "        data_list.append(str2float(tds[13].string))\n",
    "\n",
    "        data_list_per_hour.append(data_list)\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "    return data_list_per_hour\n",
    "\n",
    "def create_csv():\n",
    "    # CSV 出力先ディレクトリ\n",
    "    output_dir = '降水量/'\n",
    "\n",
    "    # 出力ファイル名\n",
    "    output_file = \"熊本.csv\"\n",
    "\n",
    "    # データ取得開始・終了日\n",
    "    start_date = datetime.date(2019, 1, 1)\n",
    "    end_date   = datetime.date(2020, 12, 31)\n",
    "\n",
    "    # CSV の列\n",
    "    fields = [\"年月日\", \"時間\", \"気圧（現地）\", \"気圧（海面）\",\n",
    "              \"降水量\", \"気温\", \"露点湿度\", \"蒸気圧\", \"湿度\",\n",
    "              \"風速\", \"風向\", \"日照時間\", \"全天日射量\", \"降雪\", \"積雪\"] # 天気、雲量、視程は今回は対象外とする\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file), 'w') as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerow(fields)\n",
    "\n",
    "        date = start_date\n",
    "        while date != end_date + datetime.timedelta(1):\n",
    "\n",
    "            # 対象url（今回は東京）\n",
    "            url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/hourly_s1.php?\" \\\n",
    "                  \"prec_no=86&block_no=47819&year=%d&month=%d&day=%d&view=\"%(date.year, date.month, date.day)\n",
    "\n",
    "            data_per_day = scraping(url, date)\n",
    "\n",
    "            for dpd in data_per_day:\n",
    "                writer.writerow(dpd)\n",
    "\n",
    "            date += datetime.timedelta(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e42727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "10\n",
    "北海道(札幌方面)\n",
    "11\n",
    "北海道(函館方面)\n",
    "12\n",
    "北海道(旭川方面)\n",
    "13\n",
    "北海道(釧路方面)\n",
    "14\n",
    "北海道(北見方面)\n",
    "\n",
    "15\t新潟県(にいがた)\t新潟(にいがた)\n",
    "16\t富山県(とやま)\t富山(とやま)\n",
    "17\t石川県(いしかわ)\t金沢(かなざわ)\n",
    "18\t福井県(ふくい)\t福井(ふくい)\n",
    "19\t山梨県(やまなし)\t甲府(こうふ)\n",
    "20\t長野県(ながの)\t長野(ながの)\n",
    "21\t岐阜県(ぎふ)\t岐阜(ぎふ)\n",
    "22\t静岡県(しずおか)\t静岡(しずおか)\n",
    "23\t愛知県(あいち)\t名古屋(なごや)\n",
    "24\t三重県(みえ)\t津(つ)\n",
    "25\t滋賀県(しが)\t大津(おおつ)\n",
    "26\t京都府(きょうと)\t京都(きょうと)\n",
    "27\t大阪府(おおさか)\t大阪(おおさか)\n",
    "28\t兵庫県(ひょうご)\t神戸(こうべ)\n",
    "29\t奈良県(なら)\t奈良(なら)\n",
    "30\t和歌山県(わかやま)\t和歌山(わかやま)\n",
    "\n",
    "山口\n",
    "広島\n",
    "岡山"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49930b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('traffic_accident_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38a1d7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>資料区分</th>\n",
       "      <th>都道府県コード</th>\n",
       "      <th>警察署等コード</th>\n",
       "      <th>本票番号</th>\n",
       "      <th>事故内容</th>\n",
       "      <th>死者数</th>\n",
       "      <th>負傷者数</th>\n",
       "      <th>路線コード</th>\n",
       "      <th>上下線</th>\n",
       "      <th>地点コード</th>\n",
       "      <th>...</th>\n",
       "      <th>damage_to_vehicle_type_b</th>\n",
       "      <th>airbag_equipment_type_a</th>\n",
       "      <th>airbag_equipment_type_b</th>\n",
       "      <th>side_airbag_equipment_type_a</th>\n",
       "      <th>side_airbag_equipment_type_b</th>\n",
       "      <th>personal_injury_type_a</th>\n",
       "      <th>personal_injury_type_b</th>\n",
       "      <th>weekday_type</th>\n",
       "      <th>holiday_type</th>\n",
       "      <th>death_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>小破</td>\n",
       "      <td>その他</td>\n",
       "      <td>装備あり作動</td>\n",
       "      <td>その他</td>\n",
       "      <td>その他</td>\n",
       "      <td>負傷</td>\n",
       "      <td>損傷なし</td>\n",
       "      <td>月</td>\n",
       "      <td>その他</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>中破</td>\n",
       "      <td>その他</td>\n",
       "      <td>その他</td>\n",
       "      <td>その他</td>\n",
       "      <td>その他</td>\n",
       "      <td>損傷なし</td>\n",
       "      <td>負傷</td>\n",
       "      <td>木</td>\n",
       "      <td>その他</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>40130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>中破</td>\n",
       "      <td>装備あり作動</td>\n",
       "      <td>その他</td>\n",
       "      <td>その他</td>\n",
       "      <td>その他</td>\n",
       "      <td>負傷</td>\n",
       "      <td>負傷</td>\n",
       "      <td>日</td>\n",
       "      <td>その他</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>中破</td>\n",
       "      <td>装備あり作動</td>\n",
       "      <td>その他</td>\n",
       "      <td>装備あり作動</td>\n",
       "      <td>その他</td>\n",
       "      <td>損傷なし</td>\n",
       "      <td>負傷</td>\n",
       "      <td>金</td>\n",
       "      <td>その他</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>小破</td>\n",
       "      <td>その他</td>\n",
       "      <td>その他</td>\n",
       "      <td>その他</td>\n",
       "      <td>その他</td>\n",
       "      <td>損傷なし</td>\n",
       "      <td>負傷</td>\n",
       "      <td>日</td>\n",
       "      <td>その他</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   資料区分  都道府県コード  警察署等コード  本票番号  事故内容  死者数  負傷者数  路線コード  上下線  地点コード  ...  \\\n",
       "0     1       10       59     1     2    0     1  40010    1      0  ...   \n",
       "1     1       10       59     2     2    0     1  40010    1      0  ...   \n",
       "2     1       10       59     3     2    0     2  40130    1      0  ...   \n",
       "3     1       10       59     4     2    0     1  40130    1      0  ...   \n",
       "4     1       10      101     1     2    0     1  39990    0      0  ...   \n",
       "\n",
       "   damage_to_vehicle_type_b  airbag_equipment_type_a  airbag_equipment_type_b  \\\n",
       "0                        小破                      その他                   装備あり作動   \n",
       "1                        中破                      その他                      その他   \n",
       "2                        中破                   装備あり作動                      その他   \n",
       "3                        中破                   装備あり作動                      その他   \n",
       "4                        小破                      その他                      その他   \n",
       "\n",
       "   side_airbag_equipment_type_a  side_airbag_equipment_type_b  \\\n",
       "0                           その他                           その他   \n",
       "1                           その他                           その他   \n",
       "2                           その他                           その他   \n",
       "3                        装備あり作動                           その他   \n",
       "4                           その他                           その他   \n",
       "\n",
       "   personal_injury_type_a  personal_injury_type_b  weekday_type  holiday_type  \\\n",
       "0                      負傷                    損傷なし             月           その他   \n",
       "1                    損傷なし                      負傷             木           その他   \n",
       "2                      負傷                      負傷             日           その他   \n",
       "3                    損傷なし                      負傷             金           その他   \n",
       "4                    損傷なし                      負傷             日           その他   \n",
       "\n",
       "   death_flag  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "\n",
       "[5 rows x 112 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1458c994",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['資料区分',\n",
       " '都道府県コード',\n",
       " '警察署等コード',\n",
       " '本票番号',\n",
       " '事故内容',\n",
       " '死者数',\n",
       " '負傷者数',\n",
       " '路線コード',\n",
       " '上下線',\n",
       " '地点コード',\n",
       " '市区町村コード',\n",
       " '発生日時\\u3000\\u3000年',\n",
       " '発生日時\\u3000\\u3000月',\n",
       " '発生日時\\u3000\\u3000日',\n",
       " '発生日時\\u3000\\u3000時',\n",
       " '発生日時\\u3000\\u3000分',\n",
       " '昼夜',\n",
       " '天候',\n",
       " '地形',\n",
       " '路面状態',\n",
       " '道路形状',\n",
       " '環状交差点の直径',\n",
       " '信号機',\n",
       " '一時停止規制\\u3000標識（当事者A）',\n",
       " '一時停止規制\\u3000表示（当事者A）',\n",
       " '一時停止規制\\u3000標識（当事者B）',\n",
       " '一時停止規制\\u3000表示（当事者B）',\n",
       " '車道幅員',\n",
       " '道路線形',\n",
       " '衝突地点',\n",
       " 'ゾーン規制',\n",
       " '中央分離帯施設等',\n",
       " '歩車道区分',\n",
       " '事故類型',\n",
       " '年齢（当事者A）',\n",
       " '年齢（当事者B）',\n",
       " '当事者種別（当事者A）',\n",
       " '当事者種別（当事者B）',\n",
       " '用途別（当事者A）',\n",
       " '用途別（当事者B）',\n",
       " '車両形状（当事者A）',\n",
       " '車両形状（当事者B）',\n",
       " '速度規制（指定のみ）（当事者A）',\n",
       " '速度規制（指定のみ）（当事者B）',\n",
       " '車両の衝突部位（当事者A）',\n",
       " '車両の衝突部位（当事者B）',\n",
       " '車両の損壊程度（当事者A）',\n",
       " '車両の損壊程度（当事者B）',\n",
       " 'エアバッグの装備（当事者A）',\n",
       " 'エアバッグの装備（当事者B）',\n",
       " 'サイドエアバッグの装備（当事者A）',\n",
       " 'サイドエアバッグの装備（当事者B）',\n",
       " '人身損傷程度（当事者A）',\n",
       " '人身損傷程度（当事者B）',\n",
       " '地点\\u3000緯度（北緯）',\n",
       " '地点\\u3000経度（東経）',\n",
       " '曜日(発生年月日)',\n",
       " '祝日(発生年月日)',\n",
       " 'accident_date',\n",
       " 'pref_name',\n",
       " 'genuine_pref_cd',\n",
       " 'genuine_pref_name',\n",
       " 'tiiki-code',\n",
       " 'ken-name',\n",
       " 'sityouson-name1',\n",
       " 'sityouson-name2',\n",
       " 'sityouson-name3',\n",
       " 'accident_type',\n",
       " 'road_cd_f4',\n",
       " 'road_cd_l1',\n",
       " 'road_type',\n",
       " 'road_bypass',\n",
       " 'road_updown_type',\n",
       " 'day_night_type',\n",
       " 'weather_type',\n",
       " 'terrain_type',\n",
       " 'road_condition_type',\n",
       " 'road_shape_type',\n",
       " 'roundabout_diameter_type',\n",
       " 'traffic_lights_type',\n",
       " 'pause_sign_type_a',\n",
       " 'pause_sign_type_b',\n",
       " 'pause_display_type_a',\n",
       " 'pause_display_type_b',\n",
       " 'road_width_type',\n",
       " 'road_alignment_type',\n",
       " 'zone_regulation_type',\n",
       " 'pedestrian_road_division_type',\n",
       " 'accident_vehicle_type',\n",
       " 'age_type_a',\n",
       " 'age_type_b',\n",
       " 'parties_type_a',\n",
       " 'parties_type_b',\n",
       " 'use_type_a',\n",
       " 'use_type_b',\n",
       " 'vehicle_shape_type_a',\n",
       " 'vehicle_shape_type_b',\n",
       " 'speed_regulation_type_a',\n",
       " 'speed_regulation_type_b',\n",
       " 'collision_site_type_a',\n",
       " 'collision_site_type_b',\n",
       " 'damage_to_vehicle_type_a',\n",
       " 'damage_to_vehicle_type_b',\n",
       " 'airbag_equipment_type_a',\n",
       " 'airbag_equipment_type_b',\n",
       " 'side_airbag_equipment_type_a',\n",
       " 'side_airbag_equipment_type_b',\n",
       " 'personal_injury_type_a',\n",
       " 'personal_injury_type_b',\n",
       " 'weekday_type',\n",
       " 'holiday_type',\n",
       " 'death_flag']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9377bac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genuine_pref_name</th>\n",
       "      <th>発生日時　　年</th>\n",
       "      <th>発生日時　　月</th>\n",
       "      <th>発生日時　　日</th>\n",
       "      <th>発生日時　　時</th>\n",
       "      <th>発生日時　　分</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>北海道</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>北海道</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>北海道</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>北海道</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>北海道</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690410</th>\n",
       "      <td>沖縄</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690411</th>\n",
       "      <td>沖縄</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690412</th>\n",
       "      <td>沖縄</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690413</th>\n",
       "      <td>沖縄</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690414</th>\n",
       "      <td>沖縄</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690415 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       genuine_pref_name  発生日時　　年  発生日時　　月  発生日時　　日  発生日時　　時  発生日時　　分\n",
       "0                    北海道     2020        1        6        7       20\n",
       "1                    北海道     2020        1       16        6       39\n",
       "2                    北海道     2020        1        5        6       15\n",
       "3                    北海道     2020        1       24       15       20\n",
       "4                    北海道     2019       12       22       18       51\n",
       "...                  ...      ...      ...      ...      ...      ...\n",
       "690410               沖縄      2019       11       26       21       10\n",
       "690411               沖縄      2019       12       24        9       25\n",
       "690412               沖縄      2019       12       26       21       28\n",
       "690413               沖縄      2019       11       27       19       55\n",
       "690414               沖縄      2019       12        5       17       37\n",
       "\n",
       "[690415 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['genuine_pref_name', '発生日時\\u3000\\u3000年',\n",
    " '発生日時\\u3000\\u3000月',\n",
    " '発生日時\\u3000\\u3000日',\n",
    " '発生日時\\u3000\\u3000時',\n",
    " '発生日時\\u3000\\u3000分',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1873572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kenmei = list(df['genuine_pref_name'])\n",
    "year = list(df['発生日時\\u3000\\u3000年'])\n",
    "month = list(df['発生日時\\u3000\\u3000月'])\n",
    "day = list(df['発生日時\\u3000\\u3000日'])\n",
    "hour = list(df['発生日時\\u3000\\u3000時'])\n",
    "minutes = list(df['発生日時\\u3000\\u3000分'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ken, year, month, day, hour, minute in zip(kenmei, year, month, day, hour, minutes):\n",
    "    print(ken, year,month,day,hour,minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "896ffb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genuine_pref_cd</th>\n",
       "      <th>tiiki-code</th>\n",
       "      <th>ken-name</th>\n",
       "      <th>sityouson-name1</th>\n",
       "      <th>sityouson-name2</th>\n",
       "      <th>sityouson-name3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1234</td>\n",
       "      <td>北海道</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>北広島市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1213</td>\n",
       "      <td>北海道</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>苫小牧市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1224</td>\n",
       "      <td>北海道</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>千歳市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1209</td>\n",
       "      <td>北海道</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>夕張市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1101</td>\n",
       "      <td>北海道</td>\n",
       "      <td>札幌市</td>\n",
       "      <td>NaN</td>\n",
       "      <td>中央区</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   genuine_pref_cd  tiiki-code ken-name sityouson-name1  sityouson-name2  \\\n",
       "0                1        1234      北海道             NaN              NaN   \n",
       "1                1        1213      北海道             NaN              NaN   \n",
       "2                1        1224      北海道             NaN              NaN   \n",
       "3                1        1209      北海道             NaN              NaN   \n",
       "4                1        1101      北海道             札幌市              NaN   \n",
       "\n",
       "  sityouson-name3  \n",
       "0            北広島市  \n",
       "1            苫小牧市  \n",
       "2             千歳市  \n",
       "3             夕張市  \n",
       "4             中央区  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['genuine_pref_cd','tiiki-code', 'ken-name',\n",
    " 'sityouson-name1',\n",
    " 'sityouson-name2',\n",
    " 'sityouson-name3',]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "254ca974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['genuine_pref_cd'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2cf37aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['北海道', '青森 ', '岩手 ', '宮城 ', '秋田 ', '山形 ', '福島 ', '東京 ', '茨城 ',\n",
       "       '栃木 ', '群馬 ', '埼玉 ', '千葉 ', '神奈川 ', '新潟 ', '山梨 ', '長野 ', '静岡 ',\n",
       "       '富山 ', '石川 ', '福井 ', '岐阜 ', '愛知 ', '三重 ', '滋賀 ', '京都 ', '大阪 ',\n",
       "       '兵庫 ', '奈良 ', '和歌山 ', '鳥取 ', '島根 ', '岡山 ', '広島 ', '山口 ', '徳島 ',\n",
       "       '香川 ', '愛媛 ', '高知 ', '福岡 ', '佐賀 ', '長崎 ', '熊本 ', '大分 ', '宮崎 ',\n",
       "       '鹿児島 ', '沖縄 '], dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ 'genuine_pref_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['tiiki-code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d3e24bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, '札幌市', '空知総合振興局', '後志総合振興局', '胆振総合振興局', '日高振興局', '渡島総合振興局',\n",
       "       '檜山振興局', '上川総合振興局', '留萌振興局', '宗谷総合振興局', '十勝総合振興局', '釧路総合振興局',\n",
       "       '根室振興局', 'オホーツク総合振興局', '上北郡', '南津軽郡', '三戸郡', '北津軽郡', '下北郡', '西津軽郡',\n",
       "       '東津軽郡', '胆沢郡', '岩手郡', '紫波郡', '上閉伊郡', '下閉伊郡', '亘理郡', '仙台市', '宮城郡',\n",
       "       '黒川郡', '柴田郡', '牡鹿郡', '加美郡', '遠田郡', '山本郡', '南秋田郡', '仙北郡', '雄勝郡',\n",
       "       '西村山郡', '東村山郡', '東田川郡', '飽海郡', '東置賜郡', '最上郡', '西置賜郡', '北村山郡',\n",
       "       '双葉郡', '伊達郡', '岩瀬郡', '西白河郡', '石川郡', '東白川郡', '田村郡', '大沼郡', '河沼郡',\n",
       "       '南会津郡', '相馬郡', '下都賀郡', '北群馬郡', '甘楽郡', '児玉郡', '北葛飾郡', '千葉市', '香取郡',\n",
       "       '横浜市', '川崎市', '相模原市', '三浦郡', '高座郡', '中郡', '足柄下郡', '足柄上郡', '愛甲郡',\n",
       "       '新潟市', '北蒲原郡', '東蒲原郡', '南魚沼郡', '中魚沼郡', '刈羽郡', '河北郡', '羽咋郡', '鹿島郡',\n",
       "       '北佐久郡', '諏訪郡', '上伊那郡', '下伊那郡', '揖斐郡', '加茂郡', '賀茂郡', '駿東郡', '名古屋市',\n",
       "       '愛知郡', '西春日井郡', '丹羽郡', '海部郡', '知多郡', '額田郡', '北設楽郡', '桑名郡', '員弁郡',\n",
       "       '三重郡', '多気郡', '度会郡', '北牟婁郡', '南牟婁郡', '犬上郡', '蒲生郡', '久世郡', '京都市',\n",
       "       '乙訓郡', '綴喜郡', '相楽郡', '船井郡', '与謝郡', '大阪市', '堺市', '三島郡', '豊能郡',\n",
       "       '泉北郡', '泉南郡', '南河内郡', '神戸市', '揖保郡', '川辺郡', '多可郡', '加古郡', '神崎郡',\n",
       "       '赤穂郡', '美方郡', '磯城郡', '吉野郡', '北葛城郡', '生駒郡', '伊都郡', '有田郡', '日高郡',\n",
       "       '西牟婁郡', '東牟婁郡', '西伯郡', '岩美郡', '東伯郡', '邑智郡', '隠岐郡', '仁多郡', '岡山市',\n",
       "       '加賀郡', '都窪郡', '浅口郡', '和気郡', '小田郡', '苫田郡', '勝田郡', '久米郡', '安芸郡',\n",
       "       '広島市', '玖珂郡', '熊毛郡', '大島郡', '名西郡', '板野郡', '美馬郡', '三好郡', '木田郡',\n",
       "       '綾歌郡', '仲多度郡', '小豆郡', '上浮穴郡', '伊予郡', '喜多郡', '北宇和郡', '南宇和郡', '吾川郡',\n",
       "       '高岡郡', '幡多郡', '長岡郡', '福岡市', '糟屋郡', '北九州市', '朝倉郡', '遠賀郡', '京都郡',\n",
       "       '築上郡', '鞍手郡', '嘉穂郡', '田川郡', '三井郡', '八女郡', '三潴郡', '神埼郡', '三養基郡',\n",
       "       '東松浦郡', '西松浦郡', '杵島郡', '西彼杵郡', '東彼杵郡', '北松浦郡', '玉名郡', '球磨郡', '熊本市',\n",
       "       '菊池郡', '阿蘇郡', '上益城郡', '下益城郡', '速見郡', '玖珠郡', '児湯郡', '北諸県郡', '西諸県郡',\n",
       "       '東諸県郡', '東臼杵郡', '西臼杵郡', '薩摩郡', '姶良郡', '曽於郡', '肝属郡', '島尻郡', '中頭郡',\n",
       "       '国頭郡', '石狩振興局', '気仙郡', '西磐井郡', '九戸郡', '本吉郡', '刈田郡', '安達郡', '久慈郡',\n",
       "       '小笠原支庁', '岩船郡', '西蒲原郡', '南蒲原郡', '佐用郡', '山辺郡', '八頭郡', '山県郡', '世羅郡',\n",
       "       '神石郡', '阿武郡', '那賀郡', '越智郡', '南松浦郡', '八代郡', '葦北郡', '伊具郡', '耶麻郡',\n",
       "       '高市郡', '鹿足郡', '豊田郡', '名東郡', '土佐郡', '出水郡', '和賀郡', '藤津郡', '天草郡',\n",
       "       '鹿角郡', '能美郡', '海草郡', '日野郡', '英田郡', '八重山郡', '中津軽郡', '北秋田郡', '西宇和郡',\n",
       "       '二戸郡', '宇陀郡', '勝浦郡', '真庭郡', '飯石郡', '香川郡'], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sityouson-name1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['sityouson-name3'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "059e5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[['発生日時　　年', '発生日時　　月','発生日時　　日', '発生日時　　時']].apply(lambda x: '{}年{}月{}日  {}時'.format(x[0], x[1], x[2], x[3]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7f25c09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            2020年1月6日  7時\n",
       "1           2020年1月16日  6時\n",
       "2            2020年1月5日  6時\n",
       "3          2020年1月24日  15時\n",
       "4         2019年12月22日  18時\n",
       "                ...       \n",
       "690410    2019年11月26日  21時\n",
       "690411     2019年12月24日  9時\n",
       "690412    2019年12月26日  21時\n",
       "690413    2019年11月27日  19時\n",
       "690414     2019年12月5日  17時\n",
       "Length: 690415, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e23ee43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2020-01-06 07:20:00\n",
       "1         2020-01-16 06:39:00\n",
       "2         2020-01-05 06:15:00\n",
       "3         2020-01-24 15:20:00\n",
       "4         2019-12-22 18:51:00\n",
       "                 ...         \n",
       "690410    2019-11-26 21:10:00\n",
       "690411    2019-12-24 09:25:00\n",
       "690412    2019-12-26 21:28:00\n",
       "690413    2019-11-27 19:55:00\n",
       "690414    2019-12-05 17:37:00\n",
       "Name: accident_date, Length: 690415, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['accident_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f74e0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fa97253c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "51a8fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "m = pd.read_csv('降水量/大阪.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0703e545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>年月日</th>\n",
       "      <th>時間</th>\n",
       "      <th>気圧（現地）</th>\n",
       "      <th>気圧（海面）</th>\n",
       "      <th>降水量</th>\n",
       "      <th>気温</th>\n",
       "      <th>露点湿度</th>\n",
       "      <th>蒸気圧</th>\n",
       "      <th>湿度</th>\n",
       "      <th>風速</th>\n",
       "      <th>風向</th>\n",
       "      <th>日照時間</th>\n",
       "      <th>全天日射量</th>\n",
       "      <th>降雪</th>\n",
       "      <th>積雪</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1020.8</td>\n",
       "      <td>1031.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1020.5</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>5.8</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1020.2</td>\n",
       "      <td>1030.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>5.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1019.4</td>\n",
       "      <td>1029.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1018.8</td>\n",
       "      <td>1029.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17539</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>20</td>\n",
       "      <td>1008.3</td>\n",
       "      <td>1018.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>47.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17540</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>21</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>1019.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-6.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17541</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>22</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>1019.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>50.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17542</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>23</td>\n",
       "      <td>1009.3</td>\n",
       "      <td>1019.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17543</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>24</td>\n",
       "      <td>1008.9</td>\n",
       "      <td>1019.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>-5.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17544 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              年月日  時間  気圧（現地）  気圧（海面）  降水量   気温  露点湿度  蒸気圧    湿度   風速  風向  \\\n",
       "0      2019-01-01   1  1020.8  1031.3  0.0  4.0  -0.8  5.8  71.0  0.4   0   \n",
       "1      2019-01-01   2  1020.5  1031.0  0.0  2.7  -0.7  5.8  78.0  1.0   0   \n",
       "2      2019-01-01   3  1020.2  1030.7  0.0  1.7  -1.4  5.5  80.0  0.7   0   \n",
       "3      2019-01-01   4  1019.4  1029.9  0.0  1.5  -1.1  5.7  83.0  0.8   0   \n",
       "4      2019-01-01   5  1018.8  1029.3  0.0  1.7  -1.2  5.6  81.0  1.8   0   \n",
       "...           ...  ..     ...     ...  ...  ...   ...  ...   ...  ...  ..   \n",
       "17539  2020-12-31  20  1008.3  1018.7  0.0  3.3  -6.9  3.6  47.0  5.1   0   \n",
       "17540  2020-12-31  21  1008.8  1019.2  0.0  3.3  -6.7  3.7  48.0  4.8   0   \n",
       "17541  2020-12-31  22  1009.2  1019.6  0.0  3.2  -6.2  3.8  50.0  4.3   0   \n",
       "17542  2020-12-31  23  1009.3  1019.7  0.0  3.1  -5.0  4.2  55.0  5.5   0   \n",
       "17543  2020-12-31  24  1008.9  1019.3  0.0  3.3  -5.3  4.1  53.0  5.5   0   \n",
       "\n",
       "       日照時間  全天日射量  降雪  積雪  \n",
       "0       0.0    0.0   0   0  \n",
       "1       0.0    0.0   0   0  \n",
       "2       0.0    0.0   0   0  \n",
       "3       0.0    0.0   0   0  \n",
       "4       0.0    0.0   0   0  \n",
       "...     ...    ...  ..  ..  \n",
       "17539   0.0    0.0   0   0  \n",
       "17540   0.0    0.0   0   0  \n",
       "17541   0.0    0.0   0   0  \n",
       "17542   0.0    0.0   0   0  \n",
       "17543   0.0    0.0   0   0  \n",
       "\n",
       "[17544 rows x 15 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613620e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
